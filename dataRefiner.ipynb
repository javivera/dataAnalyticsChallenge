{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "# Configuración necesaria para que decouple detecte .env (donde están guardadas las configuraciones)\n",
    "from decouple import config\n",
    "from decouple import AutoConfig\n",
    "\n",
    "config = AutoConfig(search_path=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá simplemente armamos una lista con todas las direcciones enteras de nuestros archivos csv\n",
    "# Decidí armarlo en una función por si a la hora de utilizarlo se quisiera cambiar la dirección raíz\n",
    "# Como predeterminado deje datasets que suele ser una dirección raíz común\n",
    "\n",
    "def directoryLister(rootdir = 'datasets/'):\n",
    "\n",
    "    filesDirs = []\n",
    "\n",
    "    for rootdir, dirs, files in os.walk(rootdir):\n",
    "        \n",
    "        for file in files:\n",
    "            \n",
    "            filesDirs.append(os.path.join(rootdir, file))\n",
    "        \n",
    "    # print(filesDirs)\n",
    "    return filesDirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá armamos una lista con todos los dataframes provenientes de los csv's\n",
    "\n",
    "def dfListCreator(fileDirs):\n",
    "    dataFrames = []\n",
    "    for j in filesDirs:\n",
    "        with open(j, 'r') as f:\n",
    "            #d_reader = csv.DictReader(f)\n",
    "\n",
    "            #get fieldnames from DictReader object and store in list\n",
    "            #headers = d_reader.fieldnames\n",
    "            \n",
    "            dataFrames.append(pd.read_csv(j))\n",
    "        f.close()\n",
    "    return dataFrames\n",
    "    # I had to use the column names, because of the bug that makes some rows one column larger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_checker(df,columns):\n",
    "    for j in columns:\n",
    "        if j not in df.columns:\n",
    "            print('{} no está entre las columnas'.format(j))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función column_checker está diseñada para evitar que un dataset malo corte el proceso y permitir así que los datasets buenos sean procesados\n",
    "# No la uso en este caso, por que no queda claro que espera Alkemy , y como solo tengo 3 datasets , terminaria perdiendo mucha información\n",
    "# Para evitar esto hice a mano los casos necesarios. Pero en un caso real, probablemente descartaría los datasets rotos, especialmente si fueran muchos\n",
    "# Dependiendo siempre de si puedo o no prescindir de la información que estos brindan\n",
    "# De todas maneras desde .env se puede elegir usar el checker o no.\n",
    "\n",
    "\n",
    "\n",
    "def columnSelector(dfList,columns, columns_alt = '[]'):\n",
    "    for j in range(0,len(dfList)):\n",
    "        dfList[j].columns = dfList[j].columns.str.lower()\n",
    "        dfList[j].columns = dfList[j].columns.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "        if CHECKER == True:\n",
    "            if column_checker(dfList[j],columns):\n",
    "                dfList[j] = dfList[j][columns]\n",
    "\n",
    "        else:\n",
    "            if 'domicilio' in dfList[j].columns:\n",
    "                dfList[j] = dfList[j][columns]\n",
    "\n",
    "                dfList[j].rename(columns = {'idprovincia':'id_provincia',\n",
    "                                            'iddepartamento': 'id_departamento',\n",
    "                                            'categoria':'categoría',\n",
    "                                            'cp':'código postal',\n",
    "                                            'telefono':'número de teléfono',}, inplace = True) \n",
    "\n",
    "            elif 'direccion' in dfList[j].columns:\n",
    "                dfList[j] = dfList[j][columns_alt]\n",
    "\n",
    "                dfList[j].rename(columns = {'idprovincia':'id_provincia',\n",
    "                                            'iddepartamento': 'id_departamento',\n",
    "                                            'categoria':'categoría',\n",
    "                                            'cp':'código postal',\n",
    "                                            'telefono':'número de teléfono',\n",
    "                                            'direccion':'domicilio'}, inplace = True)\n",
    "\n",
    "    return dfList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPipeline():\n",
    "    \n",
    "    DATASETS_ROOT_DIR = config('DATASETS_DIR')\n",
    "\n",
    "    # Obtiene direcciones de mis archivos csv en base a mi rootdir\n",
    "    fileDirs = directoryLister(DATASETS_ROOT_DIR)\n",
    "    \n",
    "    # Crea una lista con los dataframes en base a los csv\n",
    "    dfList = dfListCreator(fileDirs)\n",
    "\n",
    "    # Procesa cada dataframe para seleccionar ciertas columnas establecidas en la configuracion .env\n",
    "    columns = ['cod_loc','idprovincia','iddepartamento','categoria','provincia','localidad','nombre','domicilio','cp','telefono','mail','web']\n",
    "    columns_alt = ['cod_loc','idprovincia','iddepartamento','categoria','provincia','localidad','nombre','direccion','cp','telefono','mail','web']\n",
    "    CHECKER = config('CHECKER')\n",
    "\n",
    "    dfList = columnSelector(dfList, columns, columns_alt)\n",
    "\n",
    "    print(dfList[0].head())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesarCines(df):\n",
    "    # Descarta las columnas innecesarias, despues corrije la columna espacio_INCAA que tiene muchos espacios vacíos\n",
    "    # Finalmente transforma el tipo a enteros y luego cuenta cada apricion de cada columna , según la provincia\n",
    "    df = df[['Provincia','Butacas','Pantallas', 'espacio_INCAA']]\n",
    "    \n",
    "    df.loc[df['espacio_INCAA'] == 'SI' , ['espacio_INCAA']] = 1\n",
    "    df.loc[df['espacio_INCAA'] == 'si' , ['espacio_INCAA']] = 1\n",
    "    df.loc[df['espacio_INCAA'] != 1 , ['espacio_INCAA']] = 0\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    df['espacio_INCAA'] = df['espacio_INCAA'].astype(int)\n",
    "    df['Butacas'] = df['Butacas'].astype(int)\n",
    "    df['Pantallas'] = df['Pantallas'].astype(int)\n",
    "\n",
    "    \n",
    "    a = df.groupby(['Provincia']).sum()\n",
    "    #print(df)\n",
    "    #a = df.groupby([\"Provincia\"]).count()['']\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Butacas  Pantallas  espacio_INCAA\n",
      "Provincia                                                         \n",
      "Buenos Aires                       92722        357             19\n",
      "Catamarca                           3200         12              1\n",
      "Chaco                               2469         14              1\n",
      "Chubut                              2682         10              4\n",
      "Ciudad Autónoma de Buenos Aires    31386        153              3\n",
      "Corrientes                          3370         17              1\n",
      "Córdoba                            20799        105              2\n",
      "Entre Ríos                          4086         17              2\n",
      "Formosa                             1184          4              1\n",
      "Jujuy                               2277          5              2\n",
      "La Pampa                            2071          6              2\n",
      "La Rioja                            1477         10              1\n",
      "Mendoza                            11116         47              2\n",
      "Misiones                            2177         10              2\n",
      "Neuquén                             3959         12              3\n",
      "Río Negro                           2474         10              4\n",
      "Salta                               4665         23              2\n",
      "San Juan                            4617         22              2\n",
      "San Luis                            2601         12              0\n",
      "Santa Cruz                          1256          7              2\n",
      "Santa Fe                           20131         79              3\n",
      "Santiago del Estero                 2928         11              1\n",
      "Tierra del Fuego                    1445          6              0\n",
      "Tucumán                             5161         24              2\n"
     ]
    }
   ],
   "source": [
    "cinesDir = directoryLister('datasets/museos')\n",
    "cinesDf = dfListCreator(cinesDir)\n",
    "museosDf = procesarCines(cinesDf[-1]) # la última fecha\n",
    "museosDf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not connect to server: No such file or directory\n\tIs the server running locally and accepting\n\tconnections on Unix domain socket \"/tmp/.s.PGSQL.5432\"?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/jvera/gitRepos/dataAnalyticsChallenge/dataRefiner.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jvera/gitRepos/dataAnalyticsChallenge/dataRefiner.ipynb#ch0000015?line=0'>1</a>\u001b[0m conn \u001b[39m=\u001b[39m psycopg2\u001b[39m.\u001b[39;49mconnect(\u001b[39m\"\u001b[39;49m\u001b[39mdbname=mydb user=postgres password=postgres\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.miniconda3/envs/alkemy/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.miniconda3/envs/alkemy/lib/python3.10/site-packages/psycopg2/__init__.py?line=118'>119</a>\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///~/.miniconda3/envs/alkemy/lib/python3.10/site-packages/psycopg2/__init__.py?line=120'>121</a>\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///~/.miniconda3/envs/alkemy/lib/python3.10/site-packages/psycopg2/__init__.py?line=121'>122</a>\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    <a href='file:///~/.miniconda3/envs/alkemy/lib/python3.10/site-packages/psycopg2/__init__.py?line=122'>123</a>\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/.miniconda3/envs/alkemy/lib/python3.10/site-packages/psycopg2/__init__.py?line=123'>124</a>\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: No such file or directory\n\tIs the server running locally and accepting\n\tconnections on Unix domain socket \"/tmp/.s.PGSQL.5432\"?\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"dbname=mydb user=postgres password=postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bd98636d4a20fe018e287a83db47090b6659692ee4acf3e063777fc8b0e3952"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('alkemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
